Multi-source Masked Language Models for Audio

Problem Statement: Masked Language Models (MLM) like BERT have been successful
in Natural Language Processing and have been extended to multi-modal settings, such
as text-and-vision and audio-and-vision. Multi-source diffusion models (MSDMs) have
also been proposed to tackle multiple inference tasks in the music domain. The goal of
this project is to develop a multi-source masked language model for audio, where
masking is performed both intra-source and inter-source. This model should be based on
common quantized representations of audio, such as VQ-VAE or RQ-VAE (residual
quantization).

Task: Students will be required to:
1. Train a VQ-VAE on a music domain.
2. Adapt a transformer architecture on the quantized domain, training with masked
modeling on multi-source audio data, using the Slakh dataset or the in-house
pre-separated MTG-Jamendo dataset.
3. Benchmark the performance of the model with respect to MSDM qualitatively on
the generative tasks and quantitatively on the separation task.
4. (Advanced) Generalize the masked language model to the RQ-VAE quantization.